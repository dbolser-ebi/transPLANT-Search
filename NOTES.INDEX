#! /usr/local/bin/bash

## SHELL SCRIPT FOR INDEXING PARTNER DATA

# Quit the script if *anything* fails!
set -o errexit # set -e

# Quit the script if we forget to initalize a variatble!
set -o nounset # set -u


## POINT AT THE INDEXES
index_dir=/nfs/production/panda/ensemblgenomes/data/Plants/TransPLANT/Indexes


## PICK YOUR SERVER (Solr version 4+)

## Testing on a farm node
# url=http://ebi1-024:8983/solr
# url=http://ebi5-232:8983/solr

## Local testing
#url=http://localhost:1234/solr
#url=http://dbolser-laptop.windows.ebi.ac.uk:1234/solr

## Production
#url=http://pweb-2b:8045/solr
#url=http://pweb-2b:8046/solr
 url=http://ves-ebi-60:8045/solr



## Define some options common to all dumps, and add softCommit so that
## we don't have to call commit every so often...
common_opt="separator=%09&softCommit=true"



## FIRST DELETE?

# ...

## The simple way to delete an index is (typically) just:
#rm -rf solr/${core}/data/

## I gave up on deleting via solr, but it's actually pretty easy, for
## example:

# curl "${url}/${core}/update?commit=true" \
#     -H 'Content-type:application/xml' \
#     -d '<DELETE><QUERY>database_name:GEBIS</QUERY></DELETE>'

## Deleting the data directory on a runnin instance will probably fail
## (due to NFS locks). Note, you should delete and then restart solr
## after changing the schema for any core!



## INDEX EBI:
core=transPlant-EBI

for i in ${index_dir}/EBI/transplant.24-v0.2.tsv; do
    echo $i
    
    OPT=${common_opt}
    OPT="${OPT}&fieldnames=entry_type,database_name,db_id,db_version,name,description,url,species,feature_type,sequence_id,start_position,end_position"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo



## INDEX IPK
core=transPlant-IPK

for i in \
    ${index_dir}/IPK/*.tsv.new \
    ${index_dir}/IPK/optimas_dw_export_101013.tsv; do
    echo $i
    
    OPT=$common_opt
    OPT="${OPT}&header=true"
    OPT="${OPT}&trim=true"
    OPT="${OPT}&fieldnames=db_id,entry_type,database_name,species,description,url"
    
    ## Split the species
    OPT="${OPT}&f.species.split=true"
    OPT="${OPT}&f.species.separator=,"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo



## INDEX MIPS:
core=transPlant-MIPS

for i in ${index_dir}/MIPS/*.txt; do
    echo $i
    
    OPT=$common_opt
    OPT="${OPT}&fieldnames=db_id,entry_type,database_name,species,description,url"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo

for i in ${index_dir}/MIPS/crowsnest_refs_2014-09-01/*.tsv; do
    echo $i

    OPT=$common_opt
    OPT="${OPT}&header=true"
    OPT="${OPT}&fieldnames=entry_type,database_name,db_id,name,description,url,species,xref,feature_type,sequence_id,start_position,end_position"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo



## INDEX PAS:
core=transPlant-PAS

for i in ${index_dir}/PAS/*.tab; do
    echo $i
    
    OPT=$common_opt
    OPT="${OPT}&header=true"
    OPT="${OPT}&fieldnames=db_id,entry_type,database_name,species,description,url"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo



## INDEX URGI:
core=transPlant-URGI

#for i in ${index_dir}/URGI/Transplant_*.csv; do
for i in ${index_dir}/URGI/solr_dump.tsv; do
    echo $i
    
    OPT=$common_opt
    OPT="${OPT}&fieldnames=db_id,entry_type,database_name,species,description,url"
    
    ## Run it
    time \
        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
        -H 'Content-type:application/csv'
    
done
echo; echo



echo DONE



## Finally, we have to stick at least some data into the 'hub' index,
## so that it reports it's fields correctly...

## We use something similar to the PAS data...

## NOT SURE THAT THIS IS STILL NEEDED IN THE CURRENT SOLR VERSION!

#core=transPlant
#
#for i in ${index_dir}/null_entry.tsv; do
#    echo $i
#    
#    OPT=$common_opt
#    OPT="${OPT}&header=true"
#    OPT="${OPT}&fieldnames=db_id,entry_type,database_name,species,description,url"
#
#    ## Run it
#    time \
#        curl "${url}/${core}/update?${OPT}" --data-binary @"${i}" \
#        -H 'Content-type:application/csv'
#    
#done
#echo; echo



## Wuh?
set +e
set +u

